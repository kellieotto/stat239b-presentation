\documentclass{beamer}

 \usepackage{beamerthemesplit}
 
 
% Math macros
\newcommand{\cD}{{\mathcal D}}
\newcommand{\cF}{{\mathcal F}}
\newcommand{\todo}[1]{{\color{red}{TO DO: \sc #1}}}

\newcommand{\reals}{\mathbb{R}}
\newcommand{\integers}{\mathbb{Z}}
\newcommand{\naturals}{\mathbb{N}}
\newcommand{\rationals}{\mathbb{Q}}

\newcommand{\ind}[1]{1{\{#1\}}} % Indicator function
\newcommand{\pr}{\mathbb{P}} % Generic probability
\newcommand{\ex}{\mathbb{E}} % Generic expectation
\newcommand{\var}{\textrm{Var}}
\newcommand{\cov}{\textrm{Cov}}

\newcommand{\normal}{N} % for normal distribution (can probably skip this)
\newcommand{\eps}{\varepsilon}
\newcommand\independent{\protect\mathpalette{\protect\independenT}{\perp}}
\def\independenT#1#2{\mathrel{\rlap{$#1#2$}\mkern2mu{#1#2}}}

\newcommand{\convd}{\stackrel{d}{\longrightarrow}} % convergence in distribution/law/measure
\newcommand{\convp}{\stackrel{P}{\longrightarrow}} % convergence in probability
\newcommand{\convas}{\stackrel{\textrm{a.s.}}{\longrightarrow}} % convergence almost surely

\newcommand{\eqd}{\stackrel{d}{=}} % equal in distribution/law/measure
\newcommand{\argmax}{\arg\!\max}
\newcommand{\argmin}{\arg\!\min}
 \newcommand{\bit}{\begin{itemize}}
 \newcommand{\eit}{\end{itemize}}
 
 
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\title{A review of ``On the Failure of the Bootstrap for Matching Estimators'' (Abadie and Imbens; 2008)}
\author{Andrew Do, Kellie Ottoboni, Simon Walter}
\date{April 8, 2016}

\begin{document}

\frame{\titlepage}

\section[Outline]{}
\frame{\tableofcontents}

\section{Introduction}

\frame{
\frametitle{Problem statement}
\todo{problem of finding standard errors for an estimator

asymptotic results are sometimes available}
}

\frame
{
  \frametitle{The bootstrap}
\todo{nifty slide or two explaining the bootstrap. Mike Jordan had a nice graphic in his bag of little bootstraps talk.}
}




\section{Abadie and Imbens (2008)}

\frame{
\frametitle{On the failure of the bootstrap}
Thesis: \todo

}

\subsection{Notation and Assumptions}

\frame{
\frametitle{Notation and Assumptions}
\begin{itemize}
\item Suppose we have a random sample of $N_0$ units from the control population and a random sample of $N_1$ units from the treated population, with $N = N_0 + N_1$
\item Each unit has a pair of potential outcomes, $Y_i(0)$ and $Y_i(1)$, under the control and active treatments
\item Let $W_i$ indicate treatment: we observe $Y_i = W_i Y_i(1) + (1-W_i) Y_i(0)$
\item In addition to the outcome, we observe a (scalar) covariate $X_i$ for each individual
\end{itemize}
We're interested in the \textbf{average treatment effect for the treated} (ATT):

$$\tau = \ex(Y_i(1) - Y_i(0) \mid W_i = 1)$$
}


\frame{
\frametitle{Notation and Assumptions}

We make the usual assumptions for matching:

\begin{itemize}
\item Unconfoundedness: For almost all $x$,
$$(Y_i(0), Y_i(1)) \independent W_i \mid X_i = x \text{almost surely}$$
\item Overlap: For some $c>0$ and almost all $x$,
$$c \leq \pr(W_i = 1 \mid X_i = x) \leq 1-c$$
\end{itemize}
}



\frame{
\frametitle{Notation and Assumptions}
$D_i$ is the distance between the covariate values for observation $i$ and the closest control group match:

$$D_i = \min_{j = 1, \dots, N: W_j = 0} \left\Vert X_i - X_j \right\Vert$$

$\mathcal{J}(i)$ is the set of closest matches for treated unit $i$. 

\begin{displaymath}
   \mathcal{J}(i) = \left\{
     \begin{array}{lr}
       \{ j \in \{1, \dots, N\} : W_j = 0, \left\Vert X_i - X_j \right\Vert = D_i \} & \text{ if  } W_i = 1\\
       \emptyset & \text{ if  } W_i = 0
     \end{array}
   \right.
\end{displaymath} 

If $X$ is continuous, this set will consist of one unit with probability 1. In bootstrap samples, units may appear more than once.
}


\frame{
\frametitle{Notation and Assumptions}
Estimate the counterfactual for each treated unit as:

$$\hat{Y}_i(0) = \frac{1}{\# \mathcal{J}(i)} \sum_{j \in \mathcal{J}(i)} Y_i$$

The matching estimator of $\tau$ is then

$$\hat{\tau} = \frac{1}{N_1} \sum_{i : W_i = 1} \left(Y_i - \hat{Y}_i(0)\right)$$
}




\frame{
\frametitle{Notation and Assumptions}
An alternative way of writing the estimator is

$$\hat{\tau} = \frac{1}{N_1} \sum_{i=1}^N (W_i - (1-W_i)K_i) Y_i$$

where $K_i$ is the weighted number of times that unit $i$ is used as a match:

\begin{displaymath}
   K_i = \left\{
     \begin{array}{lr}
      0 & \text{ if  } W_i = 1\\
      \sum_{j: W_j=1} \ind{i \in \mathcal{J}(j)} \frac{1}{\#\mathcal{J}(j)} & \text{ if  } W_i = 0
     \end{array}
   \right.
\end{displaymath} 
}



\subsection{The Bootstrap}

\frame{
\frametitle{Bootstrap}
\begin{itemize}
\item Think of $Z = (X, W, Y)$ as a random sample and $t(\cdot)$ as a functional on $Z$. $\hat{\tau} = t(Z)$. 
\item We obtain a \textbf{bootstrap sample} $Z_b$ by taking a random sample with replacement from $Z$. 
\item We calculate the bootstrap estimator by applying $t(\cdot)$ to $Z_b$: $\hat{\tau}_b = t(Z_b)$.
\end{itemize}
}


\frame{
\frametitle{Bootstrap}
The bootstrap variance of $\hat{\tau}$ is the variance of $\hat{\tau}_b$ conditional on $Z$:

$$V^{B} = \ex\left[ (\hat{\tau}_b - \hat{\tau})^2 \mid Z\right]$$

We estimate it by generating $B$ bootstrap samples from $Z$ and taking the following average:

$$\hat{V}^B = \frac{1}{B} \sum_{b=1}^B \left( \hat{\tau}_b - \hat{\tau} \right)^2$$
}

\frame{
\frametitle{Bootstrap}
\textbf{Issue:} the bootstrap fails to replicate the distribution of $K_i$, even in large samples
\begin{itemize}
\item Suppose the ratio $N_1/N_0$ is small (i.e. there are few treated relative to controls)
\item In the original sample, few controls are used as a match more than once
\item In bootstrap samples, treated units may appear multiple times, creating situations where $\pr(K_{b,i} > 1) > \pr(K_i > 1)$ \todo{is this technically correct? is there a better way to put this?}
\end{itemize}
}


\frame{
\frametitle{Bootstrap}
some theory?
}

\frame{
\frametitle{Prior work on the inconsistency of the bootstrap}
\bit
\item In a 2006 version of their paper Abadie and Imbens claim  this is the first case for which the bootstrap is inconsistent for a statistic that is asymptotically normal and $\sqrt{n}$-consistent.
\item But this is not true. Beran (1982) establishes that a Hodges-type estimator for the  mean:

$$\theta(X_1, \ldots, X_n) = \begin{cases} 
b \bar{X}_n \textrm{  if $ |\bar{X}_n| < n^{-1/4}$} \\
\bar{X}_n \textrm{  if $ |\bar{X}_n| \geq n^{-1/4}$}
\end{cases}$$
is not consistently estimated by the bootstrap when the true mean is zero.
\eit
}

\frame{
\bit
\item In the final version of the paper,  there is some suggestion this is the first example for which the bootstrap is inconsistent for a statistic that is asymptotically normal, $\sqrt{n}$-consistent and asymptotically unbiased.

\item This is not true either because Beran's example is also asymptotically unbiased. Moreover it is easy to construct much simpler examples that are unbiased in finite samples too.
\item We give one here: suppose $F$ is a continuous distribution with finite mean and we want to estimate the mean.
\item Our statistic is $$\theta(\hat{F}) = \theta(X_1, \ldots, X_n) = \bar{X} + \#\{(i,j) : X_i = X_j, i \neq j \}$$
\eit
}

\frame{
\bit
\item There is a widespread but imperfect and outdated intuition that states that Efron's bootstrap consistently estimates the distribution of the statistic  if and only if the statistic is asymptotically normal. 
\item Has a rigorous proof for statistics that are linear in the data but the general case defied rigorous proof.
\item  Rigorous results for non-linear statistics seem to require that small changes in the population distribution lead to small changes in the asymptotic distribution of the statistic. 
\item For example in the Beran's (1982) example it is not hard to show this is violated:
\begin{align*}
\theta(X_1, \ldots, X_n) \Longrightarrow
N\left[\mu, \frac{1}{n} \var(X))\right] \textrm{\quad   if $E(X) \neq 0$} \\
\theta(X_1, \ldots, X_n) \Longrightarrow N\left[\mu, \frac{b^2}{n} \var(X)\right] \textrm{\quad   if $E(X) = 0$}
\end{align*}

\eit
}




\frame{
\frametitle{Should we follow Abadie and Imbens recommendation?}
\bit
\item Conclusion is  only their prior work based on asymptotic normality or subsampling  have formal justification.
\item This is not satisfying: the bootstrap is used because it is often second order correct. What does this mean?
\item For many statistics of interest we can form Edgeworth expansions of the distribution function:
$$ \pr(\sqrt{n}(\hat{\theta} - \theta_0)/\sigma \leq x) = \Phi(x) + n^{-1/2} p_1(x) \phi(x) + \ldots$$
\item The $p_j(x)$ are polynomials with coefficients determined by the moments of the statistic.
\item The bootstrap distribution is:
$$ \pr^*(\sqrt{n}(\hat{\theta} - \theta_0)/\sigma \leq x) = \Phi(x) + n^{-1/2} \hat{p}_1(x) \phi(x) + \ldots$$
\item Here the $\hat{p}_j(x)$  are $p_j(x)$ with population moments substituted for their empirical counterparts.
\eit

}


\section{Simulations}





\end{document}
